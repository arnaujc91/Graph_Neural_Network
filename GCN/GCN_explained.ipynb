{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Based on https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html\n",
    "\n",
    "- useful:\n",
    "1. https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780\n",
    "2. https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html\n",
    "3. https://arxiv.org/pdf/1609.02907.pdf\n",
    "'''\n",
    "from networkx import karate_club_graph\n",
    "from sklearn import preprocessing\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch\n",
    "import itertools\n",
    "import torch.nn as nn\n",
    "from torch_geometric.utils import from_networkx\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils import tensorboard\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. What are Graph Convolutions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 The graph Laplacian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph Laplacian is defined as:\n",
    "$$\n",
    "L \\equiv D - A \\, , \\quad D_{ij} =  \\delta_{ij}\\sum_{k=0}^{n-1}A_{k,j} = \\begin{bmatrix}\n",
    "    \\sum_{k=0}^{n-1}A_{k,0} & & 0 \\\\\n",
    "    & \\ddots & \\\\\n",
    "    0 & & \\sum_{k=0}^{n-1}A_{k,n-1}\n",
    "  \\end{bmatrix} \\, ,\n",
    "$$\n",
    "\n",
    "where $A$ is the *adjacency matrix* and $D$ is *the degree matrix*. The reasons why this is called Laplacian is because this matrix reproduces an analogous behaviour in discrete analysis as the operator $\\nabla^2$ in real analysis. Some of the examples where this is shown can be found in wikipedia or in the references.\n",
    "\n",
    "The normalized Laplacian $L \\to D^{-1/2}L D^{-1/2} = I_n - D^{-1/2}A D^{-1/2} $ is more convenient to use because it does not change the scale of the input. From now on we will use $L$ to refer to the normalized Laplacian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because $L$ is a symmetric and real valued matrix there exists an orthonormal basis of eigenvectors that diagonalize it, and therefore there is an orthogonal matrix $U$ that diagonalizes the Laplacian matrix:\n",
    "\n",
    "$$\n",
    "L = U^T \\Lambda\\, U\\, , \\quad \\Lambda = \\mathrm{diag}(\\lambda_0, \\dots, \\lambda_{n-1}) =   \\begin{bmatrix}\n",
    "    \\lambda_{0} & & 0 \\\\\n",
    "    & \\ddots & \\\\\n",
    "    0 & & \\lambda_{n-1}\n",
    "  \\end{bmatrix}\n",
    "  \\, ,\n",
    "$$\n",
    "with\n",
    "$$\n",
    " U^T U = I_n \\iff U_{ik}U_{jk} =  \\delta_{ij}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 The Graph Fourier Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently the values of the eigenvectors of the Laplacian matrix form an oscillatory behaviour emulating the oscillatory character of a wave, from there they conclude that expanding any vector in terms of the Laplacian eigenbasis is the analog of a fourier decomposition of a function. \n",
    "\n",
    "I have to say that all this looks all a bit sketchy (probably more reading is necessary) but  thats the main motivation behind the fourier transform I have found so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvectors $\\{\\mathbf{u}_l\\}_{l=0}^{n-1}$ of $L$ are known as the graph Fourier modes and its associated eigenvalues $\\{\\lambda_l\\}_{l=0}^{n-1}$ as the frequencies of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A signal $x: V \\rightarrow \\mathbb{R}$ defined on the nodes of the graph may be regarded as a vector $x \\in \\mathbb{R}^n$, where $x(i)$ is the value of $x$ at the $i^{th}$ node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivated by the above argument the Graph Fourier Transform over a function $f: V \\rightarrow \\mathbb{R}$\n",
    "is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{G F}[f]=\\hat{f}= U \\cdot f\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the matrix $U$ for small graphs is no problem, but for graphs with around 100 000 nodes is already a challenging (computationally speaking) task. Many graphs of interest in industry contain $\\sim 1\\, 000\\, 000$ nodes. Therefore alternative methods to the diagonalization of this matrix should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 The Graph Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let   $f, g: V \\rightarrow \\mathbb{R}$, the convolution between the two functions is defined as:\n",
    "\n",
    "$$\n",
    "f \\, * \\, g = U^T\\left(U f\\odot U  g\\right) \\quad \\left(= U_{ji}U_{jk}f_k U_{jl}g_l\\right) \\, ,\n",
    "$$\n",
    "\n",
    "where $\\odot $ denotes the Hadamard product as usual. Note that this definition respects the Convolution Theorem: $\\widehat{(f*g)} = \\hat{f}\\odot \\hat{g}$. \n",
    "\n",
    "If we denote $\\tilde{g} = U\\cdot g$:\n",
    "$$\n",
    "\\left(f \\, * \\, g\\right)_i =  U_{ji}U_{jk}f_k \\tilde{g}_j  =  U_{ji}\\tilde{g}_j \\delta_{jl}U_{lk}f_k  \\iff f \\, * \\, g = \n",
    "U^T\\cdot \\begin{bmatrix}\n",
    "    \\tilde{g}_{0} & & 0 \\\\\n",
    "    & \\ddots & \\\\\n",
    "    0 & & \\tilde{g}_{n-1}\n",
    "  \\end{bmatrix}\n",
    "  \\cdot \n",
    "  U\\cdot x\\, .\n",
    "$$\n",
    "\n",
    "Because $\\tilde{g}$ is the value of $g$ in the eigenbasis of the Laplacian, we can write $\\tilde{g}_i = g(\\lambda_i)$. In the literature they denote $\\tilde{g}$ by $g_\\theta$, but I donted it $\\tilde{g}$ by convenience for the calculations with indices. This function $g_\\theta$ is the filter. Finally we arrive at the final expression that is used in all the papers but it is generally not explained:\n",
    "\n",
    "$$\n",
    "x * g_\\theta = U^T g_\\theta(\\Lambda) \\,U x\\, ,\n",
    "$$\n",
    "\n",
    "where again $\\Lambda = \\mathrm{diag}(\\lambda_0, \\dots, \\lambda_{n-1})  $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we end up with the general form for a convolutional layer in a graph:\n",
    "\n",
    "$$\n",
    "X^{(k+1)} = \\phi \\left( U^T \\cdot g_{\\theta}(\\Lambda) \\cdot U \\cdot X^{(k)}\\right) \\,.\n",
    "$$\n",
    "Where in this case $X^{(k)} \\in \\mathbb{R}^{n\\times d}$, so we assume that every node has $d$ features, and $X^k$ is just a matrix where each row is a $d$-dimensional feature-vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate the filter by Chebychsev polynomials:\n",
    "\n",
    "$$\n",
    "g_\\theta(\\Lambda) \\approx \\sum_{k=0}^K \\theta_k T_k(\\tilde{\\Lambda})\\, \\implies x * g_\\theta = U^T g_\\theta(\\Lambda) \\,U x \\approx \\sum_{k=0}^K \\theta_k \\left(U^TT_k(\\tilde{\\Lambda})U \\right)x =  \\sum_{k=0}^K \\theta_k T_k(\\tilde{L}) x\n",
    "$$\n",
    "\n",
    "where $\\tilde{\\Lambda} = \\frac{2}{\\lambda_{max}}\\Lambda - I_N$ and $\\tilde{L} = \\frac{2}{\\lambda_{max}}L - I_N$.\n",
    "\n",
    "Truncating the previous expansion at $K=1$ and *assuming* $\\lambda_{max} \\approx 2$, one finds:\n",
    "\n",
    "$$\n",
    "x * g_\\theta \\approx \\theta_0 x + \\theta_1 (L-I_N)x = \\theta_0 x - \\theta_1 D^{-1/2}AD^{-1/2} x\n",
    "$$\n",
    "Now in the paper of Kipf and Welling they make the arbitrary choice of $\\theta \\equiv \\theta_0\\stackrel{!}{=}-\\theta_1$ further simplifying the convolution over the filter $g_\\theta$ as:\n",
    "\n",
    "$$\n",
    "x * g_\\theta \\approx \\theta \\left(I_N + D^{-1/2}AD^{-1/2}\\right) x\n",
    "$$\n",
    "Because the matrix inside the parenthesis has eigenvalues in the range $[0,2]$ they renormalize it once more:\n",
    "$$\n",
    "I_N + D^{-1/2}AD^{-1/2} \\to \\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2} \\, ,\n",
    "$$\n",
    "with\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tilde{D}_{ij}&\\equiv \\delta_{ij}\\sum_k \\tilde{A}_{ik}\\, ,\\\\\n",
    "\\tilde{A} &\\equiv A + I_N \\, .\n",
    "\\end{align}\n",
    "$$\n",
    "We end up with:\n",
    "\n",
    "$$\n",
    "x * g_\\theta \\approx  \\theta\\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}x \n",
    "$$\n",
    "which can be generalized, if instead of one dimensional features we have $d$ dimensional features, to:\n",
    "$$\n",
    "x * g_\\theta  \\approx \\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}X \\Theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Bibliography:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Graph Convolutions and Machine Learning (Harvard)](https://dash.harvard.edu/bitstream/handle/1/38811540/OTNESS-SENIORTHESIS-2018.pdf?sequence=1&isAllowed=y)\n",
    "- [Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering](https://papers.nips.cc/paper/6081-convolutional-neural-networks-on-graphs-with-fast-localized-spectral-filtering.pdf)\n",
    "- [Graph convolutional networks: a comprehensive review](https://link.springer.com/article/10.1186/s40649-019-0069-y)\n",
    "- [Spectral Networks and Deep Locally Connected Networks on Graphs](https://arxiv.org/pdf/1312.6203.pdf)\n",
    "- [Introduction to Graph Neural Networks](https://www.morganclaypool.com/doi/abs/10.2200/S00980ED1V01Y202001AIM045)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GCN Layers in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PyTorch](https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html) defines **message passing** through the following equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_i^{(k)} = \\gamma^{(k)} \\left( \\mathbf{x}_i^{(k-1)}, \\square_{j \\in \\mathcal{N}(i)} \\, \\phi^{(k)}\\left(\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)},\\mathbf{e}_{j,i}\\right) \\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $\\mathbf{x}^{(k-1)}_i \\in \\mathbb{R}^F$, $\\mathbf{e}_{j,i} \\in \\mathbb{R}^D$ and:\n",
    "\n",
    "- $\\mathbf{x}^{(k-1)}_i \\rightarrow  $ features of node i in layer (k-1).\n",
    "- $\\mathbf{e}_{j,i} \\rightarrow $ edge features from node 𝑗 to node 𝑖.\n",
    "- $\\square \\rightarrow$  denotes a differentiable, permutation invariant function, e.g., sum, mean or max.\n",
    "- $\\phi \\rightarrow$ is a differentiable function such as MLPs (Multi Layer Perceptrons). In the [code implementation](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/gcn_conv.html#GCNConv) of the GCN Layer it corresponds to the function **message()**.\n",
    "- $\\gamma \\rightarrow$ is a differentiable function such as MLPs (Multi Layer Perceptrons). In the [code implementation](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/message_passing.html#MessagePassing) of the GCN Layer it corresponds to the function **update()**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more clarity we could represent the indices not shown in the PyTorch docu as *greek letters*.\n",
    "\n",
    "A simple example could be:\n",
    "$\n",
    "x^{(k)}_{i,\\alpha} = \\sigma\\left(A_{ij}x^{(k-1)}_{j,\\beta}W^{(k-1)}_{\\beta\\alpha}\\right)\n",
    "$\n",
    "\n",
    "Where $\\sigma$ is a non-linear activation function such as the ReLU function.\n",
    "\n",
    "$$\n",
    "x^{(k)}_{i,\\alpha}\n",
    "\\quad\n",
    "   \\begin{cases}\n",
    "      k & \\text{is the k'th layer}\\\\\n",
    "      i, & \\text{is the number of nodes}\\\\\n",
    "      \\alpha, & \\text{is the number of features}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "$A$ is a square matrix, because we want the preserve the number of nodes. $W$ instead can be a non square matrix, such that we change the number of features of the node from layer $k$ to layer $k+1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\square \\rightarrow$  $Ax^{(k-1)}\\left(= A_{ij}x^{(k-1)}_{j,\\alpha}\\right)$\n",
    "- $\\phi \\rightarrow$ $\\phi^{(k)}\\left(\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)},\\mathbf{e}_{j,i}\\right) \\equiv \\phi^{(k)}\\left(\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)}\\right) = \\sigma\\left(Ax^{(k-1)}W^{(k-1)}\\right)$\n",
    "- $\\gamma \\rightarrow$ is just the identity function, basically it does nothing.\n",
    "\n",
    "Where in the parenthesis I have included all the indices and used Einstein summation convention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets work it out in the particular example of the GCN Layer:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_i^{(k)} = \\sum_{j \\in \\mathcal{N}(i) \\cup \\{ i \\}} \\frac{1}{\\sqrt{\\deg(i)} \\cdot \\sqrt{\\deg(j)}} \\cdot \\left( \\mathbf{\\Theta} \\cdot \\mathbf{x}_j^{(k-1)} \\right)\\, ,\\quad \\left(=  \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}} x^{(k-1)}\\mathbf{\\Theta}\\right)\n",
    "$$\n",
    "\n",
    "where $\\deg(i)$ is the degree of the node $\\rightarrow$ [Degree (graph theory)](https://en.wikipedia.org/wiki/Degree_(graph_theory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\square \\rightarrow$  $\\sum_{j \\in \\mathcal{N}(i) \\cup \\{ i \\}}$\n",
    "- $\\phi \\rightarrow$ $\\phi^{(k)}\\left(\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)},\\mathbf{e}_{j,i}\\right) \\equiv \\phi^{(k)}\\left(\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)}\\right) =  \\frac{1}{\\sqrt{\\deg(i)} \\cdot \\sqrt{\\deg(j)}} \\cdot \\left( \\mathbf{\\Theta} \\cdot \\mathbf{x}_j^{(k-1)} \\right)$\n",
    "- $\\gamma \\rightarrow$ is just the identity function, basically it does nothing.\n",
    "\n",
    "I am actually not sure about what $\\phi$ does, because the normalization is done outside of *message* and the product with the weights $\\mathbf{\\Theta}$ as well (in the PyTorch code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{x}_i^{(k)} = \\square_{j \\in \\mathcal{N}(i)} \\, \\phi^{(k)}\\left(\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)}\\right) = \\sum_{j \\in \\mathcal{N}(i) \\cup \\{ i \\}} \\frac{1}{\\sqrt{\\deg(i)} \\cdot \\sqrt{\\deg(j)}} \\cdot \\left( \\mathbf{\\Theta} \\cdot \\mathbf{x}_j^{(k-1)} \\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "zkc = karate_club_graph()\n",
    "\n",
    "data = from_networkx(zkc)\n",
    "n_features = 6\n",
    "embed = nn.Embedding(34, n_features)\n",
    "\n",
    "data.x = embed.weight\n",
    "data.y = torch.tensor(le.fit_transform(data.club), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.unique(data.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 41., 163.], grad_fn=<HistcBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x.histc(bins=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': Parameter containing:\n",
       " tensor([[-0.7382, -0.2088,  0.5928,  0.8889,  0.5277,  0.5846],\n",
       "         [ 0.4638,  2.4488,  0.1399, -0.3852, -1.8729,  1.4333],\n",
       "         [ 1.1990, -0.6522,  0.1400,  0.6717, -0.1713, -1.9192],\n",
       "         [-0.5701, -1.4612,  1.4591, -0.4475, -0.6011, -1.3340],\n",
       "         [-1.5037,  0.8464,  0.8845,  1.6852, -0.6831, -1.5835],\n",
       "         [-0.8618,  0.4507,  0.1794, -0.6434, -0.0695,  0.8467],\n",
       "         [ 0.3148, -0.3778, -0.5804,  0.0696,  1.4144,  1.9650],\n",
       "         [ 1.3317, -0.3174,  0.4669,  0.5679,  0.1118, -0.1033],\n",
       "         [-0.2857, -0.0534, -0.4106, -0.4172,  0.2771, -0.4157],\n",
       "         [ 1.5899, -0.1107, -0.3369,  0.8111,  0.1131, -0.7807],\n",
       "         [-1.4079,  0.3540, -0.8098, -0.8489, -3.7270,  0.2566],\n",
       "         [-0.5196, -0.2513, -0.2294, -0.7413, -0.1640, -0.2490],\n",
       "         [-0.7793,  1.6842, -0.4517,  0.3681, -1.0456,  0.0626],\n",
       "         [ 0.2841,  0.0798,  1.2007,  0.0566, -0.9146, -0.1025],\n",
       "         [-1.6494,  0.4369,  0.3494,  1.0768,  0.4046,  0.4628],\n",
       "         [ 0.9147, -0.3357,  0.3871, -1.2994,  1.2009, -0.5134],\n",
       "         [ 1.6674, -0.5581,  1.0628, -0.1285,  0.5649, -1.3926],\n",
       "         [ 1.4143,  0.2070,  0.2166,  1.2271,  0.5057, -0.0435],\n",
       "         [-0.4458, -0.3865, -1.6686, -0.4064, -1.6178, -0.5344],\n",
       "         [ 1.3403,  0.5325, -1.1286,  0.1000, -0.9063, -0.2282],\n",
       "         [-0.9327, -1.4566,  1.1744, -0.3927, -0.9112, -0.3043],\n",
       "         [-0.8999,  0.5045, -0.3633,  0.2887,  0.3584,  0.1625],\n",
       "         [ 0.2849,  0.5480, -0.8711, -0.8738, -0.2296,  1.0202],\n",
       "         [-0.5383, -0.8708,  0.6763,  1.4831, -0.0156, -0.4168],\n",
       "         [-0.3408, -0.0196,  0.6881,  0.8830, -0.8189, -0.1734],\n",
       "         [-0.0523,  0.6816,  0.9389, -0.1483, -1.1980,  0.0137],\n",
       "         [-0.2847, -0.4622,  1.1997, -0.3778,  0.3242,  1.3639],\n",
       "         [-0.3667,  0.0281,  1.2279,  0.2350,  1.4764,  0.5023],\n",
       "         [-0.6928, -0.0544, -0.2138, -0.2280, -0.3063, -0.0600],\n",
       "         [ 1.7144,  0.7498, -0.2800,  0.3733,  1.4096,  0.7922],\n",
       "         [ 0.3439,  0.0985,  0.6455,  0.7897,  0.1188,  1.5768],\n",
       "         [-0.1630, -0.1294, -0.9857, -2.3331, -0.0914, -1.1395],\n",
       "         [-0.5565, -0.1427, -1.1007,  1.2113, -0.1091,  0.9640],\n",
       "         [ 1.1823,  1.0895, -0.0085,  0.1512,  1.2938, -0.4810]],\n",
       "        requires_grad=True),\n",
       " 'edge_index': tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
       "           1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,\n",
       "           3,  3,  3,  3,  3,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,\n",
       "           7,  7,  8,  8,  8,  8,  8,  9,  9, 10, 10, 10, 11, 12, 12, 13, 13, 13,\n",
       "          13, 13, 14, 14, 15, 15, 16, 16, 17, 17, 18, 18, 19, 19, 19, 20, 20, 21,\n",
       "          21, 22, 22, 23, 23, 23, 23, 23, 24, 24, 24, 25, 25, 25, 26, 26, 27, 27,\n",
       "          27, 27, 28, 28, 28, 29, 29, 29, 29, 30, 30, 30, 30, 31, 31, 31, 31, 31,\n",
       "          31, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33,\n",
       "          33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33],\n",
       "         [ 1,  2,  3,  4,  5,  6,  7,  8, 10, 11, 12, 13, 17, 19, 21, 31,  0,  2,\n",
       "           3,  7, 13, 17, 19, 21, 30,  0,  1,  3,  7,  8,  9, 13, 27, 28, 32,  0,\n",
       "           1,  2,  7, 12, 13,  0,  6, 10,  0,  6, 10, 16,  0,  4,  5, 16,  0,  1,\n",
       "           2,  3,  0,  2, 30, 32, 33,  2, 33,  0,  4,  5,  0,  0,  3,  0,  1,  2,\n",
       "           3, 33, 32, 33, 32, 33,  5,  6,  0,  1, 32, 33,  0,  1, 33, 32, 33,  0,\n",
       "           1, 32, 33, 25, 27, 29, 32, 33, 25, 27, 31, 23, 24, 31, 29, 33,  2, 23,\n",
       "          24, 33,  2, 31, 33, 23, 26, 32, 33,  1,  8, 32, 33,  0, 24, 25, 28, 32,\n",
       "          33,  2,  8, 14, 15, 18, 20, 22, 23, 29, 30, 31, 33,  8,  9, 13, 14, 15,\n",
       "          18, 19, 20, 22, 23, 26, 27, 28, 29, 30, 31, 32]]),\n",
       " 'edge_attr': None,\n",
       " 'y': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "         1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'pos': None,\n",
       " 'norm': None,\n",
       " 'face': None,\n",
       " 'club': ['Mr. Hi',\n",
       "  'Mr. Hi',\n",
       "  'Mr. Hi',\n",
       "  'Mr. Hi',\n",
       "  'Mr. Hi',\n",
       "  'Mr. Hi',\n",
       "  'Mr. Hi',\n",
       "  'Mr. Hi',\n",
       "  'Mr. Hi',\n",
       "  'Officer',\n",
       "  'Mr. Hi',\n",
       "  'Mr. Hi',\n",
       "  'Mr. Hi',\n",
       "  'Mr. Hi',\n",
       "  'Officer',\n",
       "  'Officer',\n",
       "  'Mr. Hi',\n",
       "  'Mr. Hi',\n",
       "  'Officer',\n",
       "  'Mr. Hi',\n",
       "  'Officer',\n",
       "  'Mr. Hi',\n",
       "  'Officer',\n",
       "  'Officer',\n",
       "  'Officer',\n",
       "  'Officer',\n",
       "  'Officer',\n",
       "  'Officer',\n",
       "  'Officer',\n",
       "  'Officer',\n",
       "  'Officer',\n",
       "  'Officer',\n",
       "  'Officer',\n",
       "  'Officer'],\n",
       " '__num_nodes__': 34}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node feature matrix with shape [num_nodes, num_node_features]:  torch.Size([34, 6])\n",
      "\n",
      "Graph connectivity in COO format with shape [2, num_edges]:  torch.Size([2, 156])\n"
     ]
    }
   ],
   "source": [
    "print('Node feature matrix with shape [num_nodes, num_node_features]: ', data.x.shape)\n",
    "print('\\nGraph connectivity in COO format with shape [2, num_edges]: ', data.edge_index.shape)\n",
    "# print('\\nNumber of classes: ', dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(33), tensor(33))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(data.edge_index[0]), max(data.edge_index[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. The model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCNConv??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_features=2, n_hidden=2, n_output=1):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(n_features, n_hidden)\n",
    "        self.conv2 = GCNConv(n_hidden, n_output)\n",
    "\n",
    "    def forward(self, data):\n",
    "        ''' x (Tensor): Node feature matrix. Shape [num_nodes, num_node_features]'''\n",
    "        ''' edge_index (LongTensor): Graph connectivity in COO format. Shape [2, num_edges]'''\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.view(34, )\n",
    "\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(n_features=n_features).to(device)\n",
    "optimizer = torch.optim.Adam(itertools.chain(model.parameters(), embed.parameters()), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0053, -0.0312,  0.0071, -0.0277,  0.0302, -0.0191,  0.0528,  0.0823,\n",
       "         -0.0631,  0.0729,  0.0596,  0.0298,  0.0294, -0.0091,  0.0151,  0.0407],\n",
       "        [-0.0509,  0.2361, -0.0555,  0.2472, -0.0025,  0.1550, -0.0704, -0.0828,\n",
       "         -0.0008,  0.0061, -0.0575, -0.0340,  0.0319, -0.0304,  0.1290, -0.1260],\n",
       "        [-0.0963,  0.0383, -0.0828, -0.0552,  0.0221,  0.0051,  0.0631,  0.1872,\n",
       "         -0.0259, -0.0057,  0.1380,  0.1858, -0.1658,  0.0753, -0.0261, -0.0763]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._modules['conv1']._parameters['weight'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "epochs = 800\n",
    "writer = SummaryWriter(\"./runs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "save_statistics=False\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    # we are training with all the data at once, but for bigger datasets one can use a mask\n",
    "    loss = F.binary_cross_entropy(out, data.y)\n",
    "    acc = out.round().eq(data.y).sum().item()/len(data.y)\n",
    "    if epoch % 10 == 0 and save_statistics:\n",
    "        writer.add_scalar('training loss', loss, epoch)\n",
    "        writer.add_scalar('Accuracy', acc, epoch)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9706\n"
     ]
    }
   ],
   "source": [
    "acc = model(data).round().eq(data.y).sum().item()/len(data.y)\n",
    "print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir=runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
